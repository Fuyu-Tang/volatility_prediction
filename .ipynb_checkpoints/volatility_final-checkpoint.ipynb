{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "df1=pd.read_csv(\"SH000300/SH000300_2009_1.csv\")\n",
    "df2=pd.read_csv(\"SH000300/SH000300_2009_2.csv\")\n",
    "df3=pd.read_csv(\"SH000300/SH000300_2010_1.csv\")\n",
    "df4=pd.read_csv(\"SH000300/SH000300_2010_2.csv\")\n",
    "df5=pd.read_csv(\"SH000300/SH000300_2011_1.csv\")\n",
    "df6=pd.read_csv(\"SH000300/SH000300_2011_2.csv\")\n",
    "df7=pd.read_csv(\"SH000300/SH000300_2012_1.csv\")\n",
    "df8=pd.read_csv(\"SH000300/SH000300_2012_2.csv\")\n",
    "df=pd.concat([df1,df2,df3,df4,df5,df6,df7,df8],axis=0)\n",
    "\"\"\"\n",
    "df1=pd.read_csv(\"SH010107/SH010107_2006_2.csv\")\n",
    "df2=pd.read_csv(\"SH010107/SH010107_2007_1.csv\")\n",
    "df3=pd.read_csv(\"SH010107/SH010107_2007_2.csv\")\n",
    "df4=pd.read_csv(\"SH010107/SH010107_2008_1.csv\")\n",
    "df5=pd.read_csv(\"SH010107/SH010107_2008_2.csv\")\n",
    "df6=pd.read_csv(\"SH010107/SH010107_2009_1.csv\")\n",
    "df7=pd.read_csv(\"SH010107/SH010107_2009_2.csv\")\n",
    "df8=pd.read_csv(\"SH010107/SH010107_2010_1.csv\")\n",
    "df9=pd.read_csv(\"SH010107/SH010107_2010_2.csv\")\n",
    "df10=pd.read_csv(\"SH010107/SH010107_2011_1.csv\")\n",
    "df11=pd.read_csv(\"SH010107/SH010107_2011_2.csv\")\n",
    "df12=pd.read_csv(\"SH010107/SH010107_2012_1.csv\")\n",
    "df13=pd.read_csv(\"SH010107/SH010107_2012_2.csv\")\n",
    "df=pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13],axis=0)\n",
    "df=df.set_index(\"Date\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取日波动率——以DataFrame的格式\n",
    "returns=df[\"Close\"].pct_change()\n",
    "volatility=returns.groupby(\"Date\").std()\n",
    "volatility.name=\"volatility\"\n",
    "print(volatility)\n",
    "volatility.head()\n",
    "lookahead_volatility=volatility.shift(-1).dropna()\n",
    "#print(lookahead_volatility)#这才是每天对应的下一天的预测值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#后面思考这个标准化到底需不需要\n",
    "norm_data=df[[\"Open\",\"High\",\"Low\",\"Close\"]].apply(lambda x:(x-np.mean(x))/(np.std(x)))\n",
    "#norm_data=df[[\"Open\",\"High\",\"Low\",\"Close\"]].apply(lambda x:(x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "#波动率标签先暂时不进行[0,1]\n",
    "Group_data=norm_data[[\"Open\",\"High\",\"Low\",\"Close\"]].groupby(\"Date\")\n",
    "#Group_data.apply(lambda x:print(x))\n",
    "normed_volatility=(lookahead_volatility-np.mean(lookahead_volatility))/(np.std(lookahead_volatility))\n",
    "#normed_volatility=(lookahead_volatility-np.min(lookahead_volatility))/(np.max(lookahead_volatility)-np.min(lookahead_volatility))\n",
    "#normed_volatility=lookahead_volatility\n",
    "print(normed_volatility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_train_set(X,Y):\n",
    "    Xs=[]\n",
    "    for name, group in X:\n",
    "        cur_X=group[[\"Open\",\"High\",\"Low\",\"Close\"]].iloc[230:240]\n",
    "        cur_X_array=np.array(cur_X)\n",
    "        cur_X_list=cur_X_array.tolist()\n",
    "        #print(cur_X_list)\n",
    "        Xs.append(cur_X_list)\n",
    "    Xs.pop()#将2012/9/7的数据去掉。我们先试图预测一下2012/9/6的波动率。\n",
    "    return np.array(Xs), np.array(Y)\n",
    "x,y=get_train_set(Group_data, normed_volatility)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "x_train, x_test=x[:int(len(x) * .9)],x[int(len(x) * .9):-1]\n",
    "y_train, y_test=y[:int(len(y) * .9)],y[int(len(y) * .9):-1]\n",
    "#实际上我们得到的数据是今天的价格对应明天的波动率，一直到2012/9/5。事实上这是9-6的输入和输出。\n",
    "#这里留下最后一行作为预测项。我们希望预测9-7，输入9-6的价格数据，得出9-7的波动率。\n",
    "x_unknown=x[[-1]]#这是表格上2012/9/6对应的价格数据，实际上是9-7应该的输入，我们估计9-7的波动率。\n",
    "print(x_unknown.shape)\n",
    "Y_test=normed_volatility[int(len(x)* .9):]#以DataFrame格式存储以便后面可以调用索引。\n",
    "print(x_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Permute, RepeatVector\n",
    "from keras.layers import Merge, Input, concatenate\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers import Convolution1D, AtrousConvolution1D\n",
    "from keras.layers.pooling import AveragePooling1D, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras.optimizers import RMSprop, Adam, SGD, Nadam\n",
    "from keras.initializers import *\n",
    "from keras.constraints import *\n",
    "from keras import regularizers\n",
    "from keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= .5\n",
    "_data_shape = (10, 4)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(LSTM(128, input_shape=_data_shape, return_sequences=False))\n",
    "#model.add(Dropout(d))#d=0.5前面指出了\n",
    "model.add(Dense(16, kernel_initializer=('uniform'),activation='relu'))\n",
    "model.add(Dense(1,kernel_initializer='uniform',activation='tanh'))\n",
    "\n",
    "opt=Nadam(lr=0.02, clipnorm= .1)\n",
    "reduce_lr=ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=50, min_lr=0.000001, verbose=1)\n",
    "#存储模型\n",
    "checkpointer=ModelCheckpoint(monitor='val_loss',filepath='/Users/tangfuyu/Desktop/code/volatility/model_save1/model_returns.hdf5',verbose=1, save_best_only=True)\n",
    "#训练模型\n",
    "model.compile(optimizer=opt, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_data_shape = (10, 4)\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=_data_shape, return_sequences=False))\n",
    "model.add(Dense(16, kernel_initializer=('uniform'),activation='relu'))\n",
    "model.add(Dense(1,kernel_initializer='uniform',activation='tanh'))\n",
    "model.compile(optimizer=opt, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_fit是正式开始迭代数据\n",
    "history=model.fit(\n",
    "    x_train,#data\n",
    "    y_train,#labels\n",
    "    batch_size=60,#是批梯度下降和随机梯度下降的折中方法，将样本数据分为若干批。batch_size是个数。\n",
    "    epochs=10,#样本中的所有数据被计算一次就是一个epochs\n",
    "    validation_split=0.1,\n",
    "    callbacks=[reduce_lr,checkpointer],\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载模型预测\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "model.load_weights(\"/Users/tangfuyu/Desktop/experiment/pyexperiment/volatility/model_save1/model_returns.hdf5\")\n",
    "y_pred=model.predict(x_test)\n",
    "pred_df = pd.DataFrame(np.column_stack([y_test, y_pred]),columns=[\"y_test\",\"y_pred\"])\n",
    "print(pred_df)\n",
    "pred_df.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在9-7输入9-6的价格数据来获取9-7的波动率。\n",
    "y_unknown=model.predict(x_unknown)\n",
    "print(y_unknown)\n",
    "normed_predict_volatility=y_unknown[0][0]\n",
    "predict_volatility=normed_predict_volatility*np.std(lookahead_volatility)+np.mean(lookahead_volatility)\n",
    "print(predict_volatility)\n",
    "#用2012-9-6的价格序列预测2012-9-7的波动率。实际上是2012-9-6天末到2012-9-7天末的波动率估计。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取每日最高价\n",
    "high=df[\"High\"].groupby(\"Date\").max()\n",
    "high.name=\"High\"\n",
    "print(high.shape)\n",
    "#获取每日最低价\n",
    "low=df[\"Low\"].groupby(\"Date\").min()\n",
    "low.name=\"Low\"\n",
    "print(low.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以词典形式存储对应的开盘价和收盘价再转成dataframe的形式\n",
    "Open_Dict={}\n",
    "Close_Dict={}\n",
    "for name, group in df[\"Open\"].groupby(\"Date\"):\n",
    "    Open_Dict[name]=group.iloc[0]\n",
    "for name, group in df[\"Close\"].groupby(\"Date\"):\n",
    "    Close_Dict[name]=group.iloc[239]\n",
    "Open=pd.DataFrame.from_dict(Open_Dict, orient='index')\n",
    "Close=pd.DataFrame.from_dict(Close_Dict, orient='index')\n",
    "#这里形成的历史数据和前面需要的训练数据不同。这里形成的是每日的价格和对应的波动率。\n",
    "history_data=pd.concat([Open,high,low,Close,volatility],axis=1).dropna()\n",
    "history_data.columns=['Open','High','Low','Close','today_volatility']\n",
    "history=history_data.drop([\"2012/9/7\"])\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#从9-6开始我们已经得到了9月7号的估计波动率。实际上是指从9月6号结束到9月7号结束这一段时间内的波动率。\n",
    "#取包括2012-9-6往前的500个场景来统计变化。\n",
    "def get_change(dataframe,future_volatility):#要求参数为dataframe\n",
    "    Values=[]\n",
    "    Values_change=[]\n",
    "    now_value=dataframe.iloc[-1][\"Close\"]#是9-6的收盘价\n",
    "    print(now_value)\n",
    "    for i in range(len(dataframe)-500,len(dataframe),1):#得出500个变化场景。\n",
    "        after_change=dataframe.iloc[i][\"Close\"]\n",
    "        before_change=dataframe.iloc[i-1][\"Close\"]\n",
    "        volatility_before=dataframe.iloc[i][\"today_volatility\"]\n",
    "        multiple=before_change+(after_change-before_change)*future_volatility/volatility_before\n",
    "        value=now_value*multiple/before_change\n",
    "        Values.append(value)\n",
    "        value_change=value-now_value\n",
    "        Values_change.append(value_change)\n",
    "    return Values, Values_change\n",
    "\n",
    "\n",
    "Values,Values_change=get_change(history,predict_volatility)\n",
    "print(len(Values))\n",
    "print(type(Values_change))\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对500个场景进行排序，在1天展望时期以及99%的置信区间下，对应于第5个最糟糕的情形\n",
    "Values_change.sort()\n",
    "print(Values_change)\n",
    "VaR=Values_change[4]\n",
    "print(VaR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
